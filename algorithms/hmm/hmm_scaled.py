import numpy as np
from algorithms.hmm._hmm_base import HiddenMarkovModel


class HMM_Scaled(HiddenMarkovModel):
    def __init__(self, latent_variables, observations, em_dist, initial_dist=None):
        HiddenMarkovModel.__init__(
            self,
            latent_variables,
            observations,
            em_dist,
            initial_dist
        )


    def _e_step(self, seq):
        # E-Step ----------------------------------------------------------------
        # -----------------------------------------------------------------------
        # Filtering
        N = len(seq)-1
        alpha, cn = self.forward(seq)

        # Smoothing
        # probability distribution for point t in the past relative to
        # end of the sequence
        beta = self.backward(seq, cn)

        # marginal posterior dist of latent variable z_n
        # prob of being in state z at time t
        gamma = self.gamma(alpha, beta)
        xi = self.xi(alpha, beta, cn, seq)

        prob_X = self._prob_X(cn)

        return alpha, beta, gamma, prob_X, xi


    def forward_backward(self, obs_seq):
        alpha, cn = self.forward(obs_seq)
        return self._prob_X(cn)

    def _prob_X(self, cn_seq):
        return cn_seq.prod()

    def calc_cn(self, alpha_zn, cn, xnp1):
        """
        computes the probability for each observation to be the next symbol
        to be generated by the model given a sequence
        :param seq:
        :return:  1D Matrix of probabilitys for each observation
        """
        prob_X = cn.prod()
        #print(len(cn))
        # sum over all probab. of seeing future xnp1 for all
        # future states znp1
        sum0 = self.single_prob(0.)
        for idx_znp1, znp1 in enumerate(self._z):
            sum1 = self.single_prob(0.)
            for idx_zn, zn in enumerate(self._z):
                sum1 += self.prob_za_given_zb(znp1, zn)\
                        *alpha_zn[idx_zn]
            sum0 += sum1*self.prob_x_given_z(xnp1, znp1)
        return sum0#*prob_X


    def forward(self, seq, cn=None):
        """
        calculates the probability of seeing observation seq {x_1,..., x_t }
        and ending in state i \alpha

        :param seq:
        :param cn:
        :return:
        """
        alpha = self.np_zeros((len(seq),len(self._z)))
        cn = self.np_zeros((len(seq)))
        #print('-'*100)
        # calculate c1 and alpha_hat 1
        x1 = seq[0]
        for idx, zn in enumerate(self._z):
            cn[0] += self.prob_x_given_z(x1, zn)*self.prob_pi(zn)

        #if cn[0] == 0.0:
        #    print('#'*100)
        #    for idx, zn in enumerate(self._z):
        #        print('--'*10)
        #        print(self.prob_x_given_z(x1, zn))
        #        print(self.prob_pi(zn))
        #    print('#'*100)
        #    exit(-1)
        #print(cn)
        #print('-'*100)
        for idx, zn in enumerate(self._z):
            alpha[0][idx] = self.prob_pi(zn)*self.prob_x_given_z(x1,zn)\
                            *(self.single_prob(1.)/cn[0])

        #eq 13.55alpha_znm1, zn, seq, xn, cn):
        for n in range(1,len(seq)):
            #calculate cn
            xn = seq[n]
            #print('--')
            #print(n)
            cn[n] = self.calc_cn(alpha[n-1], cn[:n], xn)
            #print(cn)
            #if cn[n] == 0.0:
            if 0.0 == cn[n]:
                print('-0'*100)
                print(cn)
                s = ""
                for i, item in enumerate(cn):
                    s += str(item) + " , "
                    if i % 3  == 0: s += "\n"
                print(s)
                print('-0'*100)
                raise ValueError
            #print('--')
            for idx, zn in enumerate(self._z):
                sum = self.single_prob(0.)
                # sum over preceding alpha values of the incident states multiplicated with the transition
                # probability into the current state
                for idx_znm1, znm1 in enumerate(self._z):
                    # alpha value of prec state * prob of transitioning into current state * prob of emitting the observation
                    sum += alpha[n-1][idx_znm1] \
                           *self.prob_za_given_zb(zn, znm1)

                # multiply by the data contribution the prob of observing the current observation
                #print(sum, self.prob_x_given_z(xn, zn), cn[n])
                # todo remove cause it is unecessary
                prob_x_given_z = self.prob_x_given_z(xn, zn)
                #if prob_x_given_z == 0.0:
                #    sum = 0.0
                #else:
                sum *= prob_x_given_z*(self.single_prob(1.)/cn[n])
                alpha[n][idx] = sum

        return alpha, cn

    def nalpha_to_alpha(self, norm_alpha, cn):
        """
        computes the alpha_matrix from the given normalized alpha_matrix
        eq: (13.58)
        :param norm_alpha:
        :return: 2D array zn X len(seq)
        todo test function and confirm correct
        """
        N=len(norm_alpha)
        alpha = self.np_zeros((N,len(self._z)))

        # first compute the cumulative product values for each zn of the cns
        cumprod_cn = np.cumprod(cn)

        # multiply cumprod with corresp. alpha value
        for n in range(0, N):
            for k in range(0, len(self._z)):
                alpha[n][k] =  cumprod_cn[n]*norm_alpha[n][k]
        return alpha


    def backward(self, seq, cn):
        """
        :param sequence: {x_(t+1), x+(t+2), ..., x_(T))
        :return: the full backward matrix
        """
        # computes the probability of a sequence of future evidence for each state x_t
        # represents beta_(zn1) is the probability that z1 emitted the observations
        # beta_1(z11) | beta_(z21) | ... | beta_(zn1)=1
        # beta_1(z12) | beta_(z22) | ... | beta_(zn2)=1
        beta = self.np_zeros((len(seq), len(self._z)))
        N = len(seq)-1
        if cn is not None:
            for k in range(0, len(self._z)):
                # error location
                beta[N][k] = self.single_prob(1.)

            # start with beta_(znk) and calculate the betas backwards
            for n in range(N-1, -1, -1):
                for zn_k, zn in enumerate(self._z):
                    xnp1 = seq[n+1]
                    sum1 = self.single_prob(0.)
                    for znp1_k, znp1 in enumerate(self._z):
                        sum1 += self.prob_za_given_zb(znp1, zn) \
                               *self.prob_x_given_z(xnp1, znp1) \
                               *beta[n+1][znp1_k]
                    beta[n][zn_k] = sum1*(self.single_prob(1.)/cn[n+1])
            return beta


    def nbeta_to_beta(self, nbeta, cn):
        beta = self.np_zeros((len(nbeta), len(self._z)))
        N = len(nbeta)-1

        # first compute the cumulative product values for each zn of the cns
        # reverse cn, compute cumulative product then reverse again
        # multiply cumprod with corresp. beta value
        for k in range(0, len(self._z)):
            beta[N][k] = nbeta[N][k]

        cum_cn = self.single_prob(1.)
        for n in range(N-1, -1, -1):
            cum_cn = cn[n+1]*cum_cn
            for k in range(0, len(self._z)):
                beta[n][k] = cum_cn * nbeta[n][k]
        return beta

    def gamma(self, n_alpha, n_beta):
        return n_alpha*n_beta


    def xi(self, alpha, beta, cn, obs_seq):
        """
        xi[n][k1][k2] = probability of being
        :param alpha:
        :param beta:
        :param cn:
        :param obs_seq:
        :return:  3D array (N-1 x K x K)
        """
        N = len(obs_seq)
        K = len(self._z)

        xi = self.np_zeros((N-1, K, K))
        for n in range(1, N):
            for knm1, znm1 in enumerate(self._z):
                for kn, zn in enumerate(self._z):
                    xi[n-1][knm1][kn] = \
                        (alpha[n-1][knm1] \
                        * self.prob_x_given_z(obs_seq[n], zn) \
                        * self.prob_za_given_zb(zn, znm1) \
                        * beta[n][kn]) \
                        /cn[n]
        return xi
